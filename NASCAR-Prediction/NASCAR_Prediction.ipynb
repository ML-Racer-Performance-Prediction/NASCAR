{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "metadata": {
        "id": "TEqcz53t2oHF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = Name of the file with input data\n",
        "# Retrieves data from CSV file and splits it into train and test sets\n",
        "def get_data():\n",
        "    #link to dataset in github repo\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/ML-Racer-Performance-Prediction/NASCAR/main/Documentation/NASCAR(1905-2024(March))%201.csv\", index_col=0)\n",
        "\n",
        "    labels = data.index\n",
        "    data = data.values\n",
        "\n",
        "    #Split the dataset into input and output segments\n",
        "    driversY = labels\n",
        "    driversX = data\n",
        "\n",
        "    return driversY, driversX\n",
        "    #Divide into train and test sets and shuffle\n",
        "    #xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    #Convert data sets into tensors\n",
        "    #return (xTrain, xTest, yTrain, yTest)"
      ],
      "metadata": {
        "id": "P4Uv-O158V8a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[driversY, driversX] = get_data()\n",
        "\n",
        "print(driversX[0])\n",
        "print(driversY[0])"
      ],
      "metadata": {
        "id": "HsKAy_j6cJwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea888838-7b0b-4ef7-c01d-ee0fdbd2aa59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00000000e+00 0.00000000e+00 6.66666670e-02 9.11577000e-04\n",
            " 2.81000000e+01 2.65000000e+01 8.00000000e-01 4.66666667e-01]\n",
            "ZANE SMITH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGH0yvJ-2nah",
        "outputId": "0a35a07e-5dda-489b-a3ae-3ef488a5e033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (relu): ReLU()\n",
            "  (layer): Linear(in_features=423, out_features=128, bias=True)\n",
            "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (layer3): Linear(in_features=64, out_features=47, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# MLP model\n",
        "# Three layer MLP with relu activation functions\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer = nn.Linear(in_features = input_size, out_features = hidden_size1)\n",
        "        self.layer2 = nn.Linear(in_features = hidden_size1, out_features = hidden_size2)\n",
        "        self.layer3 = nn.Linear(in_features = hidden_size2, out_features = 35)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.layer(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        return x.sort()\n",
        "\n",
        "# Create model with hidden layers of 128 and 64 nodes\n",
        "MLP = MLP(35*10, 128, 64)\n",
        "print(MLP)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish the loss function and optimizer function to be used\n",
        "lossFunc = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(MLP.parameters(), lr = 0.001, momentum = 0.3)"
      ],
      "metadata": {
        "id": "QUbwap1C8j4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of correct values\n",
        "def correct(output, target):\n",
        "    predicted = output.argmax(1)                            # pick digit with largest network output\n",
        "    correct_ones = (predicted == target).type(torch.float)  # 1.0 for correct, 0.0 for incorrect\n",
        "    return correct_ones.sum().item()"
      ],
      "metadata": {
        "id": "Ory9BJwD8pKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_epochs = number of epochs through which the model was trained\n",
        "# loss = Cross Entropy loss of the model\n",
        "def draw_loss(epochCount, loss, name):\n",
        "    plt.plot(range(epochCount), loss, marker = \".\", markersize = 1, label = name)"
      ],
      "metadata": {
        "id": "pGvGeY-e8rdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train algorithm that takes in training DataLoader and the model\n",
        "def train(trainLoader, model):\n",
        "    # Iterate through data loader and train for every batch\n",
        "    for (data, target) in trainLoader:\n",
        "        output = MLP(data) # Output prediction of MLP\n",
        "        loss = lossFunc(output, target) # Compute loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "tza7YXsJ8tx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the default epoch count to 100\n",
        "epochCount = 100\n",
        "\n",
        "# Create lists for plotting data to compare train vs test accuracy and loss\n",
        "lossList = list()\n",
        "correctList = list()\n",
        "testLossList = list()\n",
        "testCorrectList = list()\n",
        "\n",
        "# Retrieve data and create loaders for iteration and batch creation\n",
        "[xTrain, xTest, yTrain, yTest] = get_data()\n",
        "[trainLoader, testLoader] = data_iter(1, xTrain, yTrain, xTest, yTest)\n",
        "\n",
        "# Train data for each epoch and store statistics in lists\n",
        "for i in range(epochCount):\n",
        "    train(trainLoader, MLP)\n",
        "\n",
        "    output = MLP(xTrain) # Output prediction of MLP\n",
        "    loss = lossFunc(output, yTrain) # Compute loss\n",
        "    lossList.append(loss.detach().numpy())\n",
        "    correctList.append(correct(output, yTrain)/800)\n",
        "\n",
        "    output = MLP(xTest) # Output prediction of MLP\n",
        "    loss = lossFunc(output, yTest) # Compute loss\n",
        "    testLossList.append(loss.detach().numpy())\n",
        "    testCorrectList.append(correct(output, yTest)/200)\n",
        "\n",
        "# Plot results from the model\n",
        "plt.plot()\n",
        "plt.title(\"Cross Entropy Loss vs. Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "draw_loss(epochCount, lossList, \"Train\")\n",
        "draw_loss(epochCount, testLossList, \"Test\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot()\n",
        "plt.title(\"Accuracy vs. Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "draw_loss(epochCount, correctList, \"Train\")\n",
        "draw_loss(epochCount, testCorrectList, \"Test\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "qOBWvjPT8xlb",
        "outputId": "13efe678-13a0-4e42-a240-0ade6600bc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Driver'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Driver'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c533b1ca3518>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Retrieve data and create loaders for iteration and batch creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-3bbc29b6ab2d>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#Split the dataset into input and output segments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Driver\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Driver\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Driver'"
          ]
        }
      ]
    }
  ]
}